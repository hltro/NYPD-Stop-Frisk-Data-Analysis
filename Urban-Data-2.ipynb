{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bizarre-tonight"
      },
      "source": [
        "# Assignment 4"
      ],
      "id": "bizarre-tonight"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "patent-china"
      },
      "source": [
        "**In this assignment, we will conclude our analysis of whether the stop and frisk policy was racially discriminatory, but from a very different angle than our previous mapping analysis. We will rely heavily on logistic regression and regularized regression, as discussed in class.**\n"
      ],
      "id": "patent-china"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPy-JHVGfPsF"
      },
      "source": [
        "## Data processing (5 points)\n",
        "\n",
        "**First we need to do some data processing for consistency with previous of analysis of stop_and_frisk data. Read in the CSV file \"sqf_sample.csv\" and filter for stops between 2009 and 2013 (including both 2009 and 2013 in your sample). Filter for stops of white, Black, Hispanic, and Asian pedestrians using the suspect_race column. (5 points)**\n"
      ],
      "id": "lPy-JHVGfPsF"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BWN36qCfactf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "df=pd.read_csv('/content/sqf_sample.csv')"
      ],
      "id": "BWN36qCfactf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter by year\n",
        "df = df[(df['year'] >= 2009) & (df['year'] <= 2013)]\n",
        "\n",
        "# Filter by race\n",
        "df = df[df['suspect_race'].isin([\"white\", \"black\", \"hispanic\", \"asian\"])]\n"
      ],
      "metadata": {
        "id": "9gKOia88cHzZ"
      },
      "id": "9gKOia88cHzZ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ranging-edition"
      },
      "source": [
        "## Using regression to analyze the frisk decision. (30 points)"
      ],
      "id": "ranging-edition"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "canadian-signature"
      },
      "source": [
        "**We will start by testing for racial discrimination in the decision to conduct a frisk after a stop: ie, whether minority pedestrians are more likely to be frisked (patted down for weapons) after they are stopped, controlling for other factors.**\n",
        "\n",
        "a.Using statsmodels, perform a logistic regression, using `frisked` as the dependent variable and `suspect_race` as the independent variable, to assess how the probability of being frisked after a stop varies by race. Write a few sentences interpreting the results, making sure to answer the following questions: which value of suspect_race is omitted from the regression coefficients, even though it appears in the data? Which race groups are most likely to be frisked after being stopped? How do you interpret the magnitude and sign of the coefficients? How do you interpret their statistical significance and confidence intervals? (5 points)"
      ],
      "id": "canadian-signature"
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm"
      ],
      "metadata": {
        "id": "XWsnpJ_EAcUd"
      },
      "id": "XWsnpJ_EAcUd",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "_nH3mUK5actf",
        "outputId": "d49dd907-9b28-494e-af7c-e647d37184ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.681382\n",
            "         Iterations 4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                           Logit Regression Results                           \n",
              "==============================================================================\n",
              "Dep. Variable:                frisked   No. Observations:               249802\n",
              "Model:                          Logit   Df Residuals:                   249798\n",
              "Method:                           MLE   Df Model:                            3\n",
              "Date:                Thu, 03 Nov 2022   Pseudo R-squ.:                0.005311\n",
              "Time:                        00:23:58   Log-Likelihood:            -1.7021e+05\n",
              "converged:                       True   LL-Null:                   -1.7112e+05\n",
              "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
              "===============================================================================================\n",
              "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
              "-----------------------------------------------------------------------------------------------\n",
              "Intercept                      -0.0900      0.022     -4.161      0.000      -0.132      -0.048\n",
              "C(suspect_race)[T.black]        0.4048      0.022     18.128      0.000       0.361       0.449\n",
              "C(suspect_race)[T.hispanic]     0.4267      0.023     18.726      0.000       0.382       0.471\n",
              "C(suspect_race)[T.white]       -0.1219      0.025     -4.834      0.000      -0.171      -0.072\n",
              "===============================================================================================\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Logit Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>        <td>frisked</td>     <th>  No. Observations:  </th>   <td>249802</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>249798</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>            <td>Thu, 03 Nov 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.005311</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                <td>00:23:58</td>     <th>  Log-Likelihood:    </th> <td>-1.7021e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.7112e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td> 0.000</td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "               <td></td>                  <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th>                   <td>   -0.0900</td> <td>    0.022</td> <td>   -4.161</td> <td> 0.000</td> <td>   -0.132</td> <td>   -0.048</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(suspect_race)[T.black]</th>    <td>    0.4048</td> <td>    0.022</td> <td>   18.128</td> <td> 0.000</td> <td>    0.361</td> <td>    0.449</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(suspect_race)[T.hispanic]</th> <td>    0.4267</td> <td>    0.023</td> <td>   18.726</td> <td> 0.000</td> <td>    0.382</td> <td>    0.471</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(suspect_race)[T.white]</th>    <td>   -0.1219</td> <td>    0.025</td> <td>   -4.834</td> <td> 0.000</td> <td>   -0.171</td> <td>   -0.072</td>\n",
              "</tr>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "fitted_model = sm.Logit.from_formula('frisked ~ C(suspect_race)', data=df).fit()\n",
        "fitted_model.summary()"
      ],
      "id": "_nH3mUK5actf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8WYaqSvWoRI"
      },
      "source": [
        "1. \"Asian\" is omitted from the regression coefficients, even though it appeared in data.\n",
        "\n",
        "2. Hispanic suspects are most likely to be frisked after being stoped, with a coefficient of 0.4614.\n",
        "\n",
        "3. The coefficients for \"black\" and \"hispanic\" values of the dependent variable are positive, with similar maginitude close to 0.4, which is considerably big. This means that a suspect is black or hispanic, the odds of him/her getting firsked is a lot more higher. On the other hand, the coefficient of \"white\" is negative with a relatively small maginitude. This means that if a suspect is white, the odds of him/her getting frisked is slightly lower. \n",
        "\n",
        "4. All three (\"white\", \"black\", \"hispanic\") coefficients are statistically significant, as their p-value are all less than 0.05. The 95% confidence interval of all coefficients are all narrow, which agree with the small p-value. None of them overlap 0, which means that the dependent variable are likely to have an association with those values of the independent varaible.Based on the large sample size and the long observation timeline, the results are reasonable and reliable."
      ],
      "id": "m8WYaqSvWoRI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "earned-mainstream"
      },
      "source": [
        "**b. Now perform a linear regression instead of a logistic regression using the same formula. How is the interpretation of the coefficients similar or different in the two regressions? What are the advantages of each? (5 points)**"
      ],
      "id": "earned-mainstream"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "cPbtbXKkactg",
        "outputId": "6bbc7d4c-cfcd-4150-b5f0-8f5c30c02c26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                frisked   R-squared:                       0.007\n",
              "Model:                            OLS   Adj. R-squared:                  0.007\n",
              "Method:                 Least Squares   F-statistic:                     615.8\n",
              "Date:                Thu, 03 Nov 2022   Prob (F-statistic):               0.00\n",
              "Time:                        00:23:59   Log-Likelihood:            -1.7834e+05\n",
              "No. Observations:              249802   AIC:                         3.567e+05\n",
              "Df Residuals:                  249798   BIC:                         3.567e+05\n",
              "Df Model:                           3                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "===============================================================================================\n",
              "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
              "-----------------------------------------------------------------------------------------------\n",
              "Intercept                       0.4775      0.005     89.395      0.000       0.467       0.488\n",
              "C(suspect_race)[T.black]        0.1005      0.006     18.256      0.000       0.090       0.111\n",
              "C(suspect_race)[T.hispanic]     0.1059      0.006     18.850      0.000       0.095       0.117\n",
              "C(suspect_race)[T.white]       -0.0303      0.006     -4.873      0.000      -0.042      -0.018\n",
              "==============================================================================\n",
              "Omnibus:                   904686.798   Durbin-Watson:                   1.995\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            40481.509\n",
              "Skew:                          -0.257   Prob(JB):                         0.00\n",
              "Kurtosis:                       1.096   Cond. No.                         13.2\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>         <td>frisked</td>     <th>  R-squared:         </th>  <td>   0.007</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.007</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   615.8</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Thu, 03 Nov 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>00:23:59</td>     <th>  Log-Likelihood:    </th> <td>-1.7834e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>249802</td>      <th>  AIC:               </th>  <td>3.567e+05</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>249798</td>      <th>  BIC:               </th>  <td>3.567e+05</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>     \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "               <td></td>                  <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th>                   <td>    0.4775</td> <td>    0.005</td> <td>   89.395</td> <td> 0.000</td> <td>    0.467</td> <td>    0.488</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(suspect_race)[T.black]</th>    <td>    0.1005</td> <td>    0.006</td> <td>   18.256</td> <td> 0.000</td> <td>    0.090</td> <td>    0.111</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(suspect_race)[T.hispanic]</th> <td>    0.1059</td> <td>    0.006</td> <td>   18.850</td> <td> 0.000</td> <td>    0.095</td> <td>    0.117</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(suspect_race)[T.white]</th>    <td>   -0.0303</td> <td>    0.006</td> <td>   -4.873</td> <td> 0.000</td> <td>   -0.042</td> <td>   -0.018</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>904686.798</td> <th>  Durbin-Watson:     </th> <td>   1.995</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>40481.509</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>            <td>-0.257</td>   <th>  Prob(JB):          </th> <td>    0.00</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>        <td> 1.096</td>   <th>  Cond. No.          </th> <td>    13.2</td> \n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "fitted_model = sm.OLS.from_formula('frisked ~ C(suspect_race)', data=df).fit()\n",
        "fitted_model.summary()"
      ],
      "id": "cPbtbXKkactg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgk3tioqWpJg"
      },
      "source": [
        "1. The interpretation of the signs of coefficients of linear regression is similar to that of logistic regression in that: positive coefficients indicate that a variable is associated with greater values of the output, and negative coefficients indicate that a variable is associated with smaller values. Both \"black\" and \"hispanic\" are positively correlated with \"frisked\", and \"white\" is negatively correclated with \"frisked\". The interpretation of coefficients is slightly different. For linear regression, with a unit increase of independent variable, the dependent variable increases the amount of the coefficient. \n",
        "\n",
        "2. Linear regression treats the depedent variable as a continuous value, while logistic regression treats the dependent variable as a binary value (0 or 1). The logistic regression is better when comparing different groups based on a categorical outcome, while the linear regression is better when observing how a set of independent features correlate to an outcome. In this case, the logistic regression would be a better choice because being frisked or not is a binary outcome."
      ],
      "id": "rgk3tioqWpJg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "constitutional-place"
      },
      "source": [
        "**c. The regression using only race as an independent variable is a good starting point, but it does not control for any other variables. What other variables do you think are important to control for, and why?  (3 points)**"
      ],
      "id": "constitutional-place"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Some other variables worth considering: \n",
        "\n",
        "suspect_sex - men are thought to be more violent and more likely to carry weapons\n",
        "\n",
        "suspect_age - younger suspects are more likely to carry weapons\n",
        "\n",
        "suspect_build / supsect_height - heavier/bigger suspects look more threatening\n",
        "\n",
        "time / date - the time at which the suspect is stopped can affect the possibility of them being frisked\n",
        "\n",
        "location_housing / precinct - the neighborhood in which the suspect was found could also affect the result, since police are more careful with dangerous neighborhoods"
      ],
      "metadata": {
        "id": "RwsSgNq7tUGa"
      },
      "id": "RwsSgNq7tUGa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assumed-employment"
      },
      "source": [
        "**d. Run a logistic regression where you control for both race and for the \"precinct\" variable, which encodes the police precinct in which the stop occurred. Make sure to control for precinct as a categorical, not a numerical, variable, by writing it as C(precinct) in the regression formula - why is this important to do?**\n",
        "\n",
        "How do the race coefficients change, and what does that mean? How does the interpretation of this regression differ from the regression in which you only control for race? Make one argument in favor of reporting results controlling for location, and one argument against it. (5 points) "
      ],
      "id": "assumed-employment"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j7Jy6XAWacth",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d92ff15-90b5-45dc-b41b-9fd486d02482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.660162\n",
            "         Iterations 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                           Logit Regression Results                           \n",
              "==============================================================================\n",
              "Dep. Variable:                frisked   No. Observations:               249802\n",
              "Model:                          Logit   Df Residuals:                   249722\n",
              "Method:                           MLE   Df Model:                           79\n",
              "Date:                Thu, 03 Nov 2022   Pseudo R-squ.:                 0.03629\n",
              "Time:                        00:24:09   Log-Likelihood:            -1.6491e+05\n",
              "converged:                       True   LL-Null:                   -1.7112e+05\n",
              "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
              "===============================================================================================\n",
              "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
              "-----------------------------------------------------------------------------------------------\n",
              "Intercept                      -0.6489      0.061    -10.635      0.000      -0.769      -0.529\n",
              "C(suspect_race)[T.black]        0.4578      0.024     18.849      0.000       0.410       0.505\n",
              "C(suspect_race)[T.hispanic]     0.3101      0.024     12.719      0.000       0.262       0.358\n",
              "C(suspect_race)[T.white]        0.0305      0.027      1.134      0.257      -0.022       0.083\n",
              "C(precinct)[T.5.0]              0.1716      0.081      2.117      0.034       0.013       0.330\n",
              "C(precinct)[T.6.0]              0.2822      0.079      3.594      0.000       0.128       0.436\n",
              "C(precinct)[T.7.0]              0.8584      0.078     11.071      0.000       0.706       1.010\n",
              "C(precinct)[T.9.0]              0.2968      0.071      4.189      0.000       0.158       0.436\n",
              "C(precinct)[T.10.0]             0.2543      0.081      3.144      0.002       0.096       0.413\n",
              "C(precinct)[T.13.0]            -0.1085      0.074     -1.463      0.143      -0.254       0.037\n",
              "C(precinct)[T.14.0]            -0.3188      0.065     -4.896      0.000      -0.446      -0.191\n",
              "C(precinct)[T.17.0]            -0.4345      0.100     -4.340      0.000      -0.631      -0.238\n",
              "C(precinct)[T.18.0]            -0.5150      0.082     -6.259      0.000      -0.676      -0.354\n",
              "C(precinct)[T.19.0]            -0.0915      0.075     -1.225      0.221      -0.238       0.055\n",
              "C(precinct)[T.20.0]            -0.4209      0.077     -5.462      0.000      -0.572      -0.270\n",
              "C(precinct)[T.22.0]            -0.5254      0.127     -4.153      0.000      -0.773      -0.277\n",
              "C(precinct)[T.23.0]             0.3589      0.062      5.773      0.000       0.237       0.481\n",
              "C(precinct)[T.24.0]             0.0209      0.075      0.277      0.782      -0.127       0.168\n",
              "C(precinct)[T.25.0]             0.5596      0.066      8.470      0.000       0.430       0.689\n",
              "C(precinct)[T.26.0]             0.5933      0.073      8.156      0.000       0.451       0.736\n",
              "C(precinct)[T.28.0]             0.2500      0.067      3.734      0.000       0.119       0.381\n",
              "C(precinct)[T.30.0]             0.4550      0.069      6.625      0.000       0.320       0.590\n",
              "C(precinct)[T.32.0]             1.1589      0.065     17.808      0.000       1.031       1.286\n",
              "C(precinct)[T.33.0]             0.6502      0.071      9.132      0.000       0.511       0.790\n",
              "C(precinct)[T.34.0]             0.4953      0.066      7.477      0.000       0.365       0.625\n",
              "C(precinct)[T.40.0]             0.7928      0.062     12.799      0.000       0.671       0.914\n",
              "C(precinct)[T.41.0]             0.7468      0.067     11.195      0.000       0.616       0.878\n",
              "C(precinct)[T.42.0]             1.0516      0.066     15.967      0.000       0.923       1.181\n",
              "C(precinct)[T.43.0]             0.5869      0.064      9.239      0.000       0.462       0.711\n",
              "C(precinct)[T.44.0]             1.6174      0.065     24.747      0.000       1.489       1.746\n",
              "C(precinct)[T.45.0]             0.1575      0.073      2.149      0.032       0.014       0.301\n",
              "C(precinct)[T.46.0]             1.5683      0.068     22.911      0.000       1.434       1.703\n",
              "C(precinct)[T.47.0]             0.7424      0.066     11.220      0.000       0.613       0.872\n",
              "C(precinct)[T.48.0]             1.0470      0.077     13.526      0.000       0.895       1.199\n",
              "C(precinct)[T.49.0]             0.7403      0.068     10.880      0.000       0.607       0.874\n",
              "C(precinct)[T.50.0]             0.6340      0.085      7.421      0.000       0.467       0.801\n",
              "C(precinct)[T.52.0]             1.1324      0.066     17.125      0.000       1.003       1.262\n",
              "C(precinct)[T.60.0]             0.6449      0.065      9.891      0.000       0.517       0.773\n",
              "C(precinct)[T.61.0]             0.4476      0.069      6.499      0.000       0.313       0.583\n",
              "C(precinct)[T.62.0]             0.1718      0.071      2.418      0.016       0.033       0.311\n",
              "C(precinct)[T.63.0]             0.7922      0.078     10.155      0.000       0.639       0.945\n",
              "C(precinct)[T.66.0]             0.3302      0.077      4.296      0.000       0.180       0.481\n",
              "C(precinct)[T.67.0]             0.4007      0.063      6.324      0.000       0.276       0.525\n",
              "C(precinct)[T.68.0]             0.1478      0.081      1.815      0.070      -0.012       0.307\n",
              "C(precinct)[T.69.0]             0.2784      0.069      4.044      0.000       0.143       0.413\n",
              "C(precinct)[T.70.0]             0.5390      0.064      8.431      0.000       0.414       0.664\n",
              "C(precinct)[T.71.0]             0.2656      0.068      3.894      0.000       0.132       0.399\n",
              "C(precinct)[T.72.0]             0.7361      0.072     10.285      0.000       0.596       0.876\n",
              "C(precinct)[T.73.0]             0.2253      0.060      3.740      0.000       0.107       0.343\n",
              "C(precinct)[T.75.0]             0.4728      0.060      7.924      0.000       0.356       0.590\n",
              "C(precinct)[T.76.0]            -0.1059      0.072     -1.471      0.141      -0.247       0.035\n",
              "C(precinct)[T.77.0]             0.3723      0.064      5.804      0.000       0.247       0.498\n",
              "C(precinct)[T.78.0]            -0.1391      0.080     -1.729      0.084      -0.297       0.019\n",
              "C(precinct)[T.79.0]             0.2962      0.062      4.805      0.000       0.175       0.417\n",
              "C(precinct)[T.81.0]             0.7454      0.065     11.514      0.000       0.619       0.872\n",
              "C(precinct)[T.83.0]             0.7430      0.064     11.589      0.000       0.617       0.869\n",
              "C(precinct)[T.84.0]            -0.1077      0.074     -1.464      0.143      -0.252       0.036\n",
              "C(precinct)[T.88.0]             0.2522      0.068      3.705      0.000       0.119       0.386\n",
              "C(precinct)[T.90.0]             0.0350      0.064      0.552      0.581      -0.089       0.160\n",
              "C(precinct)[T.94.0]             0.7150      0.092      7.739      0.000       0.534       0.896\n",
              "C(precinct)[T.100.0]            0.3287      0.074      4.457      0.000       0.184       0.473\n",
              "C(precinct)[T.101.0]            1.1866      0.066     18.081      0.000       1.058       1.315\n",
              "C(precinct)[T.102.0]            0.7537      0.067     11.177      0.000       0.622       0.886\n",
              "C(precinct)[T.103.0]            0.3857      0.062      6.192      0.000       0.264       0.508\n",
              "C(precinct)[T.104.0]            1.1921      0.070     17.029      0.000       1.055       1.329\n",
              "C(precinct)[T.105.0]            0.5528      0.065      8.462      0.000       0.425       0.681\n",
              "C(precinct)[T.106.0]            0.4544      0.068      6.729      0.000       0.322       0.587\n",
              "C(precinct)[T.107.0]            0.4175      0.070      5.931      0.000       0.280       0.555\n",
              "C(precinct)[T.108.0]            0.5915      0.070      8.440      0.000       0.454       0.729\n",
              "C(precinct)[T.109.0]            0.9737      0.065     15.060      0.000       0.847       1.100\n",
              "C(precinct)[T.110.0]            1.5544      0.067     23.219      0.000       1.423       1.686\n",
              "C(precinct)[T.111.0]            0.6638      0.073      9.061      0.000       0.520       0.807\n",
              "C(precinct)[T.112.0]            0.7600      0.079      9.631      0.000       0.605       0.915\n",
              "C(precinct)[T.113.0]            0.5617      0.065      8.692      0.000       0.435       0.688\n",
              "C(precinct)[T.114.0]            0.6959      0.065     10.718      0.000       0.569       0.823\n",
              "C(precinct)[T.115.0]            1.4698      0.065     22.489      0.000       1.342       1.598\n",
              "C(precinct)[T.120.0]            0.1930      0.061      3.141      0.002       0.073       0.314\n",
              "C(precinct)[T.121.0]            1.1339      0.241      4.712      0.000       0.662       1.606\n",
              "C(precinct)[T.122.0]            0.3389      0.068      4.984      0.000       0.206       0.472\n",
              "C(precinct)[T.123.0]            0.2363      0.088      2.672      0.008       0.063       0.410\n",
              "===============================================================================================\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Logit Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>        <td>frisked</td>     <th>  No. Observations:  </th>   <td>249802</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>249722</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    79</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>            <td>Thu, 03 Nov 2022</td> <th>  Pseudo R-squ.:     </th>   <td>0.03629</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                <td>00:24:09</td>     <th>  Log-Likelihood:    </th> <td>-1.6491e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.7112e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td> 0.000</td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "               <td></td>                  <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th>                   <td>   -0.6489</td> <td>    0.061</td> <td>  -10.635</td> <td> 0.000</td> <td>   -0.769</td> <td>   -0.529</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(suspect_race)[T.black]</th>    <td>    0.4578</td> <td>    0.024</td> <td>   18.849</td> <td> 0.000</td> <td>    0.410</td> <td>    0.505</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(suspect_race)[T.hispanic]</th> <td>    0.3101</td> <td>    0.024</td> <td>   12.719</td> <td> 0.000</td> <td>    0.262</td> <td>    0.358</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(suspect_race)[T.white]</th>    <td>    0.0305</td> <td>    0.027</td> <td>    1.134</td> <td> 0.257</td> <td>   -0.022</td> <td>    0.083</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.5.0]</th>          <td>    0.1716</td> <td>    0.081</td> <td>    2.117</td> <td> 0.034</td> <td>    0.013</td> <td>    0.330</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.6.0]</th>          <td>    0.2822</td> <td>    0.079</td> <td>    3.594</td> <td> 0.000</td> <td>    0.128</td> <td>    0.436</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.7.0]</th>          <td>    0.8584</td> <td>    0.078</td> <td>   11.071</td> <td> 0.000</td> <td>    0.706</td> <td>    1.010</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.9.0]</th>          <td>    0.2968</td> <td>    0.071</td> <td>    4.189</td> <td> 0.000</td> <td>    0.158</td> <td>    0.436</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.10.0]</th>         <td>    0.2543</td> <td>    0.081</td> <td>    3.144</td> <td> 0.002</td> <td>    0.096</td> <td>    0.413</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.13.0]</th>         <td>   -0.1085</td> <td>    0.074</td> <td>   -1.463</td> <td> 0.143</td> <td>   -0.254</td> <td>    0.037</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.14.0]</th>         <td>   -0.3188</td> <td>    0.065</td> <td>   -4.896</td> <td> 0.000</td> <td>   -0.446</td> <td>   -0.191</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.17.0]</th>         <td>   -0.4345</td> <td>    0.100</td> <td>   -4.340</td> <td> 0.000</td> <td>   -0.631</td> <td>   -0.238</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.18.0]</th>         <td>   -0.5150</td> <td>    0.082</td> <td>   -6.259</td> <td> 0.000</td> <td>   -0.676</td> <td>   -0.354</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.19.0]</th>         <td>   -0.0915</td> <td>    0.075</td> <td>   -1.225</td> <td> 0.221</td> <td>   -0.238</td> <td>    0.055</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.20.0]</th>         <td>   -0.4209</td> <td>    0.077</td> <td>   -5.462</td> <td> 0.000</td> <td>   -0.572</td> <td>   -0.270</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.22.0]</th>         <td>   -0.5254</td> <td>    0.127</td> <td>   -4.153</td> <td> 0.000</td> <td>   -0.773</td> <td>   -0.277</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.23.0]</th>         <td>    0.3589</td> <td>    0.062</td> <td>    5.773</td> <td> 0.000</td> <td>    0.237</td> <td>    0.481</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.24.0]</th>         <td>    0.0209</td> <td>    0.075</td> <td>    0.277</td> <td> 0.782</td> <td>   -0.127</td> <td>    0.168</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.25.0]</th>         <td>    0.5596</td> <td>    0.066</td> <td>    8.470</td> <td> 0.000</td> <td>    0.430</td> <td>    0.689</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.26.0]</th>         <td>    0.5933</td> <td>    0.073</td> <td>    8.156</td> <td> 0.000</td> <td>    0.451</td> <td>    0.736</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.28.0]</th>         <td>    0.2500</td> <td>    0.067</td> <td>    3.734</td> <td> 0.000</td> <td>    0.119</td> <td>    0.381</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.30.0]</th>         <td>    0.4550</td> <td>    0.069</td> <td>    6.625</td> <td> 0.000</td> <td>    0.320</td> <td>    0.590</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.32.0]</th>         <td>    1.1589</td> <td>    0.065</td> <td>   17.808</td> <td> 0.000</td> <td>    1.031</td> <td>    1.286</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.33.0]</th>         <td>    0.6502</td> <td>    0.071</td> <td>    9.132</td> <td> 0.000</td> <td>    0.511</td> <td>    0.790</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.34.0]</th>         <td>    0.4953</td> <td>    0.066</td> <td>    7.477</td> <td> 0.000</td> <td>    0.365</td> <td>    0.625</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.40.0]</th>         <td>    0.7928</td> <td>    0.062</td> <td>   12.799</td> <td> 0.000</td> <td>    0.671</td> <td>    0.914</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.41.0]</th>         <td>    0.7468</td> <td>    0.067</td> <td>   11.195</td> <td> 0.000</td> <td>    0.616</td> <td>    0.878</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.42.0]</th>         <td>    1.0516</td> <td>    0.066</td> <td>   15.967</td> <td> 0.000</td> <td>    0.923</td> <td>    1.181</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.43.0]</th>         <td>    0.5869</td> <td>    0.064</td> <td>    9.239</td> <td> 0.000</td> <td>    0.462</td> <td>    0.711</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.44.0]</th>         <td>    1.6174</td> <td>    0.065</td> <td>   24.747</td> <td> 0.000</td> <td>    1.489</td> <td>    1.746</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.45.0]</th>         <td>    0.1575</td> <td>    0.073</td> <td>    2.149</td> <td> 0.032</td> <td>    0.014</td> <td>    0.301</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.46.0]</th>         <td>    1.5683</td> <td>    0.068</td> <td>   22.911</td> <td> 0.000</td> <td>    1.434</td> <td>    1.703</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.47.0]</th>         <td>    0.7424</td> <td>    0.066</td> <td>   11.220</td> <td> 0.000</td> <td>    0.613</td> <td>    0.872</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.48.0]</th>         <td>    1.0470</td> <td>    0.077</td> <td>   13.526</td> <td> 0.000</td> <td>    0.895</td> <td>    1.199</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.49.0]</th>         <td>    0.7403</td> <td>    0.068</td> <td>   10.880</td> <td> 0.000</td> <td>    0.607</td> <td>    0.874</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.50.0]</th>         <td>    0.6340</td> <td>    0.085</td> <td>    7.421</td> <td> 0.000</td> <td>    0.467</td> <td>    0.801</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.52.0]</th>         <td>    1.1324</td> <td>    0.066</td> <td>   17.125</td> <td> 0.000</td> <td>    1.003</td> <td>    1.262</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.60.0]</th>         <td>    0.6449</td> <td>    0.065</td> <td>    9.891</td> <td> 0.000</td> <td>    0.517</td> <td>    0.773</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.61.0]</th>         <td>    0.4476</td> <td>    0.069</td> <td>    6.499</td> <td> 0.000</td> <td>    0.313</td> <td>    0.583</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.62.0]</th>         <td>    0.1718</td> <td>    0.071</td> <td>    2.418</td> <td> 0.016</td> <td>    0.033</td> <td>    0.311</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.63.0]</th>         <td>    0.7922</td> <td>    0.078</td> <td>   10.155</td> <td> 0.000</td> <td>    0.639</td> <td>    0.945</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.66.0]</th>         <td>    0.3302</td> <td>    0.077</td> <td>    4.296</td> <td> 0.000</td> <td>    0.180</td> <td>    0.481</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.67.0]</th>         <td>    0.4007</td> <td>    0.063</td> <td>    6.324</td> <td> 0.000</td> <td>    0.276</td> <td>    0.525</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.68.0]</th>         <td>    0.1478</td> <td>    0.081</td> <td>    1.815</td> <td> 0.070</td> <td>   -0.012</td> <td>    0.307</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.69.0]</th>         <td>    0.2784</td> <td>    0.069</td> <td>    4.044</td> <td> 0.000</td> <td>    0.143</td> <td>    0.413</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.70.0]</th>         <td>    0.5390</td> <td>    0.064</td> <td>    8.431</td> <td> 0.000</td> <td>    0.414</td> <td>    0.664</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.71.0]</th>         <td>    0.2656</td> <td>    0.068</td> <td>    3.894</td> <td> 0.000</td> <td>    0.132</td> <td>    0.399</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.72.0]</th>         <td>    0.7361</td> <td>    0.072</td> <td>   10.285</td> <td> 0.000</td> <td>    0.596</td> <td>    0.876</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.73.0]</th>         <td>    0.2253</td> <td>    0.060</td> <td>    3.740</td> <td> 0.000</td> <td>    0.107</td> <td>    0.343</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.75.0]</th>         <td>    0.4728</td> <td>    0.060</td> <td>    7.924</td> <td> 0.000</td> <td>    0.356</td> <td>    0.590</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.76.0]</th>         <td>   -0.1059</td> <td>    0.072</td> <td>   -1.471</td> <td> 0.141</td> <td>   -0.247</td> <td>    0.035</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.77.0]</th>         <td>    0.3723</td> <td>    0.064</td> <td>    5.804</td> <td> 0.000</td> <td>    0.247</td> <td>    0.498</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.78.0]</th>         <td>   -0.1391</td> <td>    0.080</td> <td>   -1.729</td> <td> 0.084</td> <td>   -0.297</td> <td>    0.019</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.79.0]</th>         <td>    0.2962</td> <td>    0.062</td> <td>    4.805</td> <td> 0.000</td> <td>    0.175</td> <td>    0.417</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.81.0]</th>         <td>    0.7454</td> <td>    0.065</td> <td>   11.514</td> <td> 0.000</td> <td>    0.619</td> <td>    0.872</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.83.0]</th>         <td>    0.7430</td> <td>    0.064</td> <td>   11.589</td> <td> 0.000</td> <td>    0.617</td> <td>    0.869</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.84.0]</th>         <td>   -0.1077</td> <td>    0.074</td> <td>   -1.464</td> <td> 0.143</td> <td>   -0.252</td> <td>    0.036</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.88.0]</th>         <td>    0.2522</td> <td>    0.068</td> <td>    3.705</td> <td> 0.000</td> <td>    0.119</td> <td>    0.386</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.90.0]</th>         <td>    0.0350</td> <td>    0.064</td> <td>    0.552</td> <td> 0.581</td> <td>   -0.089</td> <td>    0.160</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.94.0]</th>         <td>    0.7150</td> <td>    0.092</td> <td>    7.739</td> <td> 0.000</td> <td>    0.534</td> <td>    0.896</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.100.0]</th>        <td>    0.3287</td> <td>    0.074</td> <td>    4.457</td> <td> 0.000</td> <td>    0.184</td> <td>    0.473</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.101.0]</th>        <td>    1.1866</td> <td>    0.066</td> <td>   18.081</td> <td> 0.000</td> <td>    1.058</td> <td>    1.315</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.102.0]</th>        <td>    0.7537</td> <td>    0.067</td> <td>   11.177</td> <td> 0.000</td> <td>    0.622</td> <td>    0.886</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.103.0]</th>        <td>    0.3857</td> <td>    0.062</td> <td>    6.192</td> <td> 0.000</td> <td>    0.264</td> <td>    0.508</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.104.0]</th>        <td>    1.1921</td> <td>    0.070</td> <td>   17.029</td> <td> 0.000</td> <td>    1.055</td> <td>    1.329</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.105.0]</th>        <td>    0.5528</td> <td>    0.065</td> <td>    8.462</td> <td> 0.000</td> <td>    0.425</td> <td>    0.681</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.106.0]</th>        <td>    0.4544</td> <td>    0.068</td> <td>    6.729</td> <td> 0.000</td> <td>    0.322</td> <td>    0.587</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.107.0]</th>        <td>    0.4175</td> <td>    0.070</td> <td>    5.931</td> <td> 0.000</td> <td>    0.280</td> <td>    0.555</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.108.0]</th>        <td>    0.5915</td> <td>    0.070</td> <td>    8.440</td> <td> 0.000</td> <td>    0.454</td> <td>    0.729</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.109.0]</th>        <td>    0.9737</td> <td>    0.065</td> <td>   15.060</td> <td> 0.000</td> <td>    0.847</td> <td>    1.100</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.110.0]</th>        <td>    1.5544</td> <td>    0.067</td> <td>   23.219</td> <td> 0.000</td> <td>    1.423</td> <td>    1.686</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.111.0]</th>        <td>    0.6638</td> <td>    0.073</td> <td>    9.061</td> <td> 0.000</td> <td>    0.520</td> <td>    0.807</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.112.0]</th>        <td>    0.7600</td> <td>    0.079</td> <td>    9.631</td> <td> 0.000</td> <td>    0.605</td> <td>    0.915</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.113.0]</th>        <td>    0.5617</td> <td>    0.065</td> <td>    8.692</td> <td> 0.000</td> <td>    0.435</td> <td>    0.688</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.114.0]</th>        <td>    0.6959</td> <td>    0.065</td> <td>   10.718</td> <td> 0.000</td> <td>    0.569</td> <td>    0.823</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.115.0]</th>        <td>    1.4698</td> <td>    0.065</td> <td>   22.489</td> <td> 0.000</td> <td>    1.342</td> <td>    1.598</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.120.0]</th>        <td>    0.1930</td> <td>    0.061</td> <td>    3.141</td> <td> 0.002</td> <td>    0.073</td> <td>    0.314</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.121.0]</th>        <td>    1.1339</td> <td>    0.241</td> <td>    4.712</td> <td> 0.000</td> <td>    0.662</td> <td>    1.606</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.122.0]</th>        <td>    0.3389</td> <td>    0.068</td> <td>    4.984</td> <td> 0.000</td> <td>    0.206</td> <td>    0.472</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(precinct)[T.123.0]</th>        <td>    0.2363</td> <td>    0.088</td> <td>    2.672</td> <td> 0.008</td> <td>    0.063</td> <td>    0.410</td>\n",
              "</tr>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "fitted_model = sm.Logit.from_formula('frisked ~ C(suspect_race) + C(precinct)', data=df).fit()\n",
        "fitted_model.summary()"
      ],
      "id": "j7Jy6XAWacth"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xddOFV6kWvzy"
      },
      "source": [
        "1. We want to treat precinct as a categorical variable because it even though its value is numerical, it represents an area not a continuous number, and therefore not linearly correlated.\n",
        "\n",
        "2. One change is that the coefficient for \"white\" became positive, which means that the odds of a suspect being frisked is higher if the suspect is white. This doesn't agree with the results when we only consider \"suspect_race\". Another change is that black becomes the race group that is most likely to be frisked. If we only control \"suspect_race\", hispanic suspects are more likely to be frisked.\n",
        "\n",
        "3. Argument in favor of controlling location: There might be dangerous neighborhoods that the police are more careful about, making suspects stopped in those areas more likely to be frisked. In the results, for a lot of the precincts, the coefficients are large and the p-values are small, which means that suspects were more likely to be frisked if they were stopped in those areas. This indicates an association between location and \"frisked\".\n",
        "\n",
        "4. Argument against controlling location: Although in there might be an association between \"location\" and \"frisked\", it does not mean that locatoin determines \"frisked\". For instance, an area might have a higher population concentration of a certain racial group, which cause it to have a higher \"frisked\" value associated with it. Race is still the only determinator, but since location is associated with race, we might get misleading result."
      ],
      "id": "xddOFV6kWvzy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expected-whole"
      },
      "source": [
        "**e. In a few sentences, explain why it is a BAD idea, conceptually, to control for the following variables if we are trying to assess whether the police racially discriminate in whom they frisk after a stop: a) \"found.weapon\", which encodes whether the frisk found a weapon, and b) \"suspect.eye\" and \"suspect.hair\", which encode the suspect's eye and hair color. (4 points)**"
      ],
      "id": "expected-whole"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5dWS0LAW6De"
      },
      "source": [
        "1. \"found.weapon\": This variable indicates whether or not weapon is found after the suspect is frisked, which is unrelated to why he/she is frisked.\n",
        "2. \"suspect.eye\" and \"suspect.hair\" might relate to \"frisk\" because certain races share similar traits (e.g. black and hispanic are more likely to have dark hair and eye color). But this is only because they are correlated to \"suspect_race\", and it does no mean that eye and hair color will determine the possibility of a suspect being frisked."
      ],
      "id": "_5dWS0LAW6De"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "closed-difference"
      },
      "source": [
        "**f. In a few sentences, explain the problem of omitted variable bias in this analysis, and how it would undermine the conclusions. (4 points)**"
      ],
      "id": "closed-difference"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74cAqUPgW7Mu"
      },
      "source": [
        "Omitted variable bias occurs whenever we fail to control for something which is correlated with the independent variable of interest and helps determine the outcome. When this happens, the causal effect from the omitted variable becomes tangled up in the coefficient on the variable with which it is correlated, which undermines our ability to infer causality and severely impacts our results. \n",
        "\n",
        "One example is \"precinct\". When we control location as variable, black suspects are most likely to be frisked, which is different from the result when we do not control location. Another example is if we take \"suspect.eye\" and \"suspect.hair\" into account and omit \"suspect_race\". The results might show that eye and hair color determine the possibility of being frisked, but it is because \"race\" is omitted.\n"
      ],
      "id": "74cAqUPgW7Mu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBwIUjuRt5o2"
      },
      "source": [
        "**g. In a few sentences, explain why only examining whether someone is frisked after a stop might fail to provide a full picture of discrimination in the stop and frisk policy. (4 points)**"
      ],
      "id": "mBwIUjuRt5o2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuBEAzCVW7zQ"
      },
      "source": [
        "When doing the analysis, we should also take the reason why the suspect is stopped into consideration. For instance, if suspects in a certain racial group is frisked more often than that of another group, given that they are all stopped for the same reason, then it means that race determines the possibility of being frisked. We might also want to look at the frequency of suspects being stopped and the outcome of the frisks for a full picture of the analysis."
      ],
      "id": "IuBEAzCVW7zQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grateful-chassis"
      },
      "source": [
        "## Outcome analysis using regularized regression (65 points)"
      ],
      "id": "grateful-chassis"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scientific-franchise"
      },
      "source": [
        "Because of the issues with omitted variables in analyses like the one above, *outcome* tests are often used: these look not at the rate at which a decision is made (like the decision to frisk), but at the outcome of the decision (for example, if the frisk is conducted to find a weapon, does it actually find one?) Now we will use an outcome-style analysis. Specifically, we will fit a machine learning model to predict the probability that each stop which was conducted on suspicion the pedestrian possessed a weapon actually finds a weapon. Stops which are very unlikely to find a weapon arguably violate the Fourth Amendment, which prohibits unreasonable searches; if such stops disproportionately occur of certain race groups, the policy may violate the Fourteenth Amendment, which prohibits racial discrimination. "
      ],
      "id": "scientific-franchise"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CaMrTP0NlPY"
      },
      "source": [
        "**a. In this portion of this analysis, you will be using a smaller version of the data to speed up model fitting. Read in the CSV file \"small_sqf_sample.csv\". As before, we need to do some data processing for consistency with previous of analysis of this data. Load in the data and filter for stops between 2009 and 2013 (including both 2009 and 2013); filter for stops of white, Black, Hispanic, and Asian pedestrians using the suspect_race column; and filter for stops conducted on suspicion of criminal posession of a weapon (ie, suspected_crime == 'cpw'). (5 points)**"
      ],
      "id": "-CaMrTP0NlPY"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SsTfY6BOacti"
      },
      "outputs": [],
      "source": [
        "df2=pd.read_csv('/content/small_sqf_sample.csv')"
      ],
      "id": "SsTfY6BOacti"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter by year\n",
        "df2 = df2[(df2['year'] >= 2009) & (df2['year'] <= 2013)]\n",
        "\n",
        "# Filter by race\n",
        "df2 = df2[df2['suspect_race'].isin([\"white\", \"black\", \"hispanic\", \"asian\"])]\n",
        "\n",
        "# Filter by stop reason\n",
        "df2 = df2[df2[\"suspected_crime\"]=='cpw']"
      ],
      "metadata": {
        "id": "wg2Nd8M1C1xp"
      },
      "id": "wg2Nd8M1C1xp",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "confident-array"
      },
      "source": [
        "\n",
        "**b. We will be fitting the regression model** \n",
        "\n",
        "`found_weapon ~ C(precinct) * C(suspect_race) + C(location_housing) + C(year) + suspect_age + suspect_height + suspect_weight + suspect_sex + ADDITIONAL_CIRCUMSTANCE_COLUMNS` \n",
        "\n",
        "where ADDITIONAL_CIRCUMSTANCE_COLUMNS are any columns that begin with \"stopped_bc\" or \"additional_\" besides \"additional_other\" and \"stopped_bc_other\". You can get these columns by running\n",
        "\n",
        "`ADDITIONAL_CIRCUMSTANCE_COLUMNS = [a for a in d.columns if ('stopped_bc' in a or 'additional_' in a) and a not in (['additional_other', 'stopped_bc_other'])]`\n",
        "\n",
        "You should have 18 additional columns. These columns provide more information about the circumstances of the stop, and we include them for consistency with the original analysis and because they turn out to be important for predictive performance. \n",
        "\n",
        "Drop any rows with missing values in any of the variables you need. \n",
        "\n",
        "Now, we need to put the data into a format which sklearn can use later - ie, numpy arrays. Do this with \"patsy\" library and the \"dmatrix\" function. You can call dmatrix as follows:\n",
        "\n",
        "`sqf_X = patsy.dmatrix('C(precinct) * C(suspect_race) + C(location_housing) + C(year) + suspect_age + suspect_height + suspect_weight + suspect_sex +' + '+'.join(ADDITIONAL_CIRCUMSTANCE_COLUMNS),sqf_data, return_type='dataframe')`\n",
        "\n",
        "and it will return a dataframe on which you can fit a regression model. The first argument to dmatrix is the formula that you want to use to make the dataframe; the second argument gives patsy the data you want to use; return_type='dataframe' ensures that you get a dataframe, not a patsy object which is hard to use.\n",
        "\n",
        "Look at the output of dmatrix and explain what the columns mean. Why can't we just pass the columns from the original dataframe directly into the sklearn function? (8 points)\n"
      ],
      "id": "confident-array"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5goZiuwFacti"
      },
      "outputs": [],
      "source": [
        "# Get additional columns that begin with \"stopped_bc\" or \"additional_\" besides \"additional_other\" and \"stopped_bc_other\"\n",
        "ADDITIONAL_CIRCUMSTANCE_COLUMNS = [a for a in df2.columns if ('stopped_bc' in a or 'additional_' in a) and a not in (['additional_other', 'stopped_bc_other'])]"
      ],
      "id": "5goZiuwFacti"
    },
    {
      "cell_type": "code",
      "source": [
        "ADDITIONAL_CIRCUMSTANCE_COLUMNS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ifyu1bw5IxE",
        "outputId": "422baa91-5471-4a11-b610-5954a0c71bde"
      },
      "id": "3ifyu1bw5IxE",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['stopped_bc_object',\n",
              " 'stopped_bc_desc',\n",
              " 'stopped_bc_casing',\n",
              " 'stopped_bc_lookout',\n",
              " 'stopped_bc_clothing',\n",
              " 'stopped_bc_drugs',\n",
              " 'stopped_bc_furtive',\n",
              " 'stopped_bc_violent',\n",
              " 'stopped_bc_bulge',\n",
              " 'additional_report',\n",
              " 'additional_investigation',\n",
              " 'additional_proximity',\n",
              " 'additional_evasive',\n",
              " 'additional_associating',\n",
              " 'additional_direction',\n",
              " 'additional_highcrime',\n",
              " 'additional_time',\n",
              " 'additional_sights']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping missing values\n",
        "df2_dropped = df2.dropna(subset=[\n",
        " 'precinct',\n",
        " 'suspect_race',\n",
        " 'found_weapon',\n",
        " 'location_housing',\n",
        " 'year',\n",
        " 'suspect_age',\n",
        " 'suspect_weight',\n",
        " 'suspect_height',\n",
        " 'suspect_sex',\n",
        " 'stopped_bc_object',\n",
        " 'stopped_bc_desc',\n",
        " 'stopped_bc_casing',\n",
        " 'stopped_bc_lookout',\n",
        " 'stopped_bc_clothing',\n",
        " 'stopped_bc_drugs',\n",
        " 'stopped_bc_furtive',\n",
        " 'stopped_bc_violent',\n",
        " 'stopped_bc_bulge',\n",
        " 'additional_report',\n",
        " 'additional_investigation',\n",
        " 'additional_proximity',\n",
        " 'additional_evasive',\n",
        " 'additional_associating',\n",
        " 'additional_direction',\n",
        " 'additional_highcrime',\n",
        " 'additional_time',\n",
        " 'additional_sights'\n",
        " ])"
      ],
      "metadata": {
        "id": "NuR9J1G96kAh"
      },
      "id": "NuR9J1G96kAh",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import patsy\n",
        "from patsy import dmatrix\n",
        "\n",
        "sqf_X = patsy.dmatrix('C(precinct) * C(suspect_race) + C(location_housing) + C(year) + suspect_age + suspect_height + suspect_weight + suspect_sex +' + '+'.join(ADDITIONAL_CIRCUMSTANCE_COLUMNS),df2_dropped, return_type='dataframe')"
      ],
      "metadata": {
        "id": "l7pDO3549YLa"
      },
      "id": "l7pDO3549YLa",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqf_X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "HUMHUuqN-gjD",
        "outputId": "c0ad34fe-5375-4c0e-8005-dec6b8a52fe4"
      },
      "id": "HUMHUuqN-gjD",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Intercept  C(precinct)[T.5.0]  C(precinct)[T.6.0]  C(precinct)[T.7.0]  \\\n",
              "0             1.0                 0.0                 0.0                 0.0   \n",
              "17            1.0                 0.0                 0.0                 0.0   \n",
              "20            1.0                 0.0                 0.0                 0.0   \n",
              "23            1.0                 0.0                 0.0                 0.0   \n",
              "26            1.0                 0.0                 0.0                 0.0   \n",
              "...           ...                 ...                 ...                 ...   \n",
              "204287        1.0                 0.0                 0.0                 0.0   \n",
              "204289        1.0                 0.0                 0.0                 0.0   \n",
              "204291        1.0                 0.0                 0.0                 0.0   \n",
              "204298        1.0                 0.0                 0.0                 0.0   \n",
              "204303        1.0                 0.0                 0.0                 0.0   \n",
              "\n",
              "        C(precinct)[T.9.0]  C(precinct)[T.10.0]  C(precinct)[T.13.0]  \\\n",
              "0                      0.0                  0.0                  0.0   \n",
              "17                     0.0                  0.0                  0.0   \n",
              "20                     0.0                  0.0                  0.0   \n",
              "23                     0.0                  0.0                  0.0   \n",
              "26                     0.0                  0.0                  0.0   \n",
              "...                    ...                  ...                  ...   \n",
              "204287                 0.0                  0.0                  0.0   \n",
              "204289                 0.0                  0.0                  0.0   \n",
              "204291                 0.0                  0.0                  0.0   \n",
              "204298                 0.0                  0.0                  0.0   \n",
              "204303                 0.0                  0.0                  0.0   \n",
              "\n",
              "        C(precinct)[T.14.0]  C(precinct)[T.17.0]  C(precinct)[T.18.0]  ...  \\\n",
              "0                       0.0                  0.0                  0.0  ...   \n",
              "17                      0.0                  0.0                  0.0  ...   \n",
              "20                      0.0                  0.0                  0.0  ...   \n",
              "23                      0.0                  0.0                  0.0  ...   \n",
              "26                      0.0                  0.0                  0.0  ...   \n",
              "...                     ...                  ...                  ...  ...   \n",
              "204287                  0.0                  0.0                  0.0  ...   \n",
              "204289                  0.0                  0.0                  0.0  ...   \n",
              "204291                  0.0                  0.0                  0.0  ...   \n",
              "204298                  0.0                  0.0                  0.0  ...   \n",
              "204303                  0.0                  0.0                  0.0  ...   \n",
              "\n",
              "        C(precinct)[T.113.0]:C(suspect_race)[T.white]  \\\n",
              "0                                                 0.0   \n",
              "17                                                0.0   \n",
              "20                                                0.0   \n",
              "23                                                0.0   \n",
              "26                                                0.0   \n",
              "...                                               ...   \n",
              "204287                                            0.0   \n",
              "204289                                            0.0   \n",
              "204291                                            0.0   \n",
              "204298                                            0.0   \n",
              "204303                                            0.0   \n",
              "\n",
              "        C(precinct)[T.114.0]:C(suspect_race)[T.white]  \\\n",
              "0                                                 0.0   \n",
              "17                                                0.0   \n",
              "20                                                0.0   \n",
              "23                                                0.0   \n",
              "26                                                0.0   \n",
              "...                                               ...   \n",
              "204287                                            0.0   \n",
              "204289                                            0.0   \n",
              "204291                                            0.0   \n",
              "204298                                            0.0   \n",
              "204303                                            0.0   \n",
              "\n",
              "        C(precinct)[T.115.0]:C(suspect_race)[T.white]  \\\n",
              "0                                                 0.0   \n",
              "17                                                0.0   \n",
              "20                                                0.0   \n",
              "23                                                0.0   \n",
              "26                                                0.0   \n",
              "...                                               ...   \n",
              "204287                                            0.0   \n",
              "204289                                            0.0   \n",
              "204291                                            0.0   \n",
              "204298                                            0.0   \n",
              "204303                                            0.0   \n",
              "\n",
              "        C(precinct)[T.120.0]:C(suspect_race)[T.white]  \\\n",
              "0                                                 0.0   \n",
              "17                                                0.0   \n",
              "20                                                0.0   \n",
              "23                                                0.0   \n",
              "26                                                0.0   \n",
              "...                                               ...   \n",
              "204287                                            0.0   \n",
              "204289                                            0.0   \n",
              "204291                                            0.0   \n",
              "204298                                            0.0   \n",
              "204303                                            0.0   \n",
              "\n",
              "        C(precinct)[T.121.0]:C(suspect_race)[T.white]  \\\n",
              "0                                                 0.0   \n",
              "17                                                0.0   \n",
              "20                                                0.0   \n",
              "23                                                0.0   \n",
              "26                                                0.0   \n",
              "...                                               ...   \n",
              "204287                                            0.0   \n",
              "204289                                            0.0   \n",
              "204291                                            0.0   \n",
              "204298                                            0.0   \n",
              "204303                                            0.0   \n",
              "\n",
              "        C(precinct)[T.122.0]:C(suspect_race)[T.white]  \\\n",
              "0                                                 0.0   \n",
              "17                                                0.0   \n",
              "20                                                0.0   \n",
              "23                                                0.0   \n",
              "26                                                0.0   \n",
              "...                                               ...   \n",
              "204287                                            0.0   \n",
              "204289                                            0.0   \n",
              "204291                                            0.0   \n",
              "204298                                            0.0   \n",
              "204303                                            0.0   \n",
              "\n",
              "        C(precinct)[T.123.0]:C(suspect_race)[T.white]  suspect_age  \\\n",
              "0                                                 0.0         49.0   \n",
              "17                                                0.0         30.0   \n",
              "20                                                0.0         31.0   \n",
              "23                                                0.0         16.0   \n",
              "26                                                0.0         42.0   \n",
              "...                                               ...          ...   \n",
              "204287                                            0.0         29.0   \n",
              "204289                                            0.0         24.0   \n",
              "204291                                            0.0         23.0   \n",
              "204298                                            0.0         16.0   \n",
              "204303                                            0.0         40.0   \n",
              "\n",
              "        suspect_height  suspect_weight  \n",
              "0             6.000000           170.0  \n",
              "17            5.583333           180.0  \n",
              "20            6.333333           270.0  \n",
              "23            5.500000           160.0  \n",
              "26            5.583333           240.0  \n",
              "...                ...             ...  \n",
              "204287        5.583333           160.0  \n",
              "204289        6.083333           170.0  \n",
              "204291        5.833333           170.0  \n",
              "204298        5.750000           170.0  \n",
              "204303        5.250000           175.0  \n",
              "\n",
              "[32100 rows x 336 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a766e23-65fd-4a22-abde-b5ddbbb7a34e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Intercept</th>\n",
              "      <th>C(precinct)[T.5.0]</th>\n",
              "      <th>C(precinct)[T.6.0]</th>\n",
              "      <th>C(precinct)[T.7.0]</th>\n",
              "      <th>C(precinct)[T.9.0]</th>\n",
              "      <th>C(precinct)[T.10.0]</th>\n",
              "      <th>C(precinct)[T.13.0]</th>\n",
              "      <th>C(precinct)[T.14.0]</th>\n",
              "      <th>C(precinct)[T.17.0]</th>\n",
              "      <th>C(precinct)[T.18.0]</th>\n",
              "      <th>...</th>\n",
              "      <th>C(precinct)[T.113.0]:C(suspect_race)[T.white]</th>\n",
              "      <th>C(precinct)[T.114.0]:C(suspect_race)[T.white]</th>\n",
              "      <th>C(precinct)[T.115.0]:C(suspect_race)[T.white]</th>\n",
              "      <th>C(precinct)[T.120.0]:C(suspect_race)[T.white]</th>\n",
              "      <th>C(precinct)[T.121.0]:C(suspect_race)[T.white]</th>\n",
              "      <th>C(precinct)[T.122.0]:C(suspect_race)[T.white]</th>\n",
              "      <th>C(precinct)[T.123.0]:C(suspect_race)[T.white]</th>\n",
              "      <th>suspect_age</th>\n",
              "      <th>suspect_height</th>\n",
              "      <th>suspect_weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>170.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>5.583333</td>\n",
              "      <td>180.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>6.333333</td>\n",
              "      <td>270.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>160.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>5.583333</td>\n",
              "      <td>240.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204287</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>5.583333</td>\n",
              "      <td>160.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204289</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6.083333</td>\n",
              "      <td>170.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204291</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>5.833333</td>\n",
              "      <td>170.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204298</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5.750000</td>\n",
              "      <td>170.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204303</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>175.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32100 rows  336 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a766e23-65fd-4a22-abde-b5ddbbb7a34e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1a766e23-65fd-4a22-abde-b5ddbbb7a34e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1a766e23-65fd-4a22-abde-b5ddbbb7a34e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dmatrix converted/encoded all the textual information (true or false) in each column into numerical values of 0 and 1. For example, if we are looking for data regarding precinct 10, all columns about precinct 10 stop and frisks will show 1 but columns for other precincts will show 0. This is very helpful given that we are doing logistic regression later on in this problem. Logistic regression need numerical values for model fitting."
      ],
      "metadata": {
        "id": "RYo3uwk6-sEW"
      },
      "id": "RYo3uwk6-sEW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAib9e-WfUdF"
      },
      "source": [
        "**c. As discussed in class, when fitting machine learning models, you should always divide the dataset into a train, val, and test set. Randomly divide the filtered, processed data into three pieces - the train set (60%), the val set (20%) and the test set (20%). (5 points)**"
      ],
      "id": "AAib9e-WfUdF"
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "RtFFt_efacti"
      },
      "outputs": [],
      "source": [
        "# Train set\n",
        "sqf_train = sqf_X.sample(frac=0.6)\n",
        "\n",
        "# Validation set\n",
        "sqf_validation_1 = sqf_X.drop(sqf_train.index)\n",
        "sqf_validation_2 = sqf_validation_1.sample(frac=0.5)\n",
        "\n",
        "# Test set\n",
        "sqf_test = sqf_validation_1.drop(sqf_validation_2.index)\n",
        "sqf_test_copy = sqf_test.copy()"
      ],
      "id": "RtFFt_efacti"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dependent variables from the found_weapon column for the next question\n",
        "train_found_weapon = df2_dropped['found_weapon'].loc[sqf_train.index.values]\n",
        "validation_found_weapon = df2_dropped['found_weapon'].loc[sqf_validation_2.index.values]\n",
        "test_found_weapon = df2_dropped['found_weapon'].loc[sqf_test.index.values]"
      ],
      "metadata": {
        "id": "QA95NcMi3P3o"
      },
      "id": "QA95NcMi3P3o",
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWZp2Jw7TKVm"
      },
      "source": [
        "**d. We will be training a regularized logistic regression model to predict the outcome. When using many machine learning models, including regularized logistic regression, it is important to preprocess the input features so they are all on the same scale, for reasons discussed in class. In this case, we will take each column in the input data, subtract its mean, and divide by its standard deviation. This makes it so each column of the data has mean 0 and standard deviation 1.**\n",
        "\n",
        "When scaling the data, it is important to compute the scaling using only the train set, as shown in class. The reason is that we are pretending that the train data is all we have access to to fit our model fitting pipeline, so we cannot \"peek\" at the validation or test sets to generate our scaling. Use sklearn's StandardScaler (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to fit a scaling transform on the train set and transform the train set (you can use the fit_transform method on the train set).Then apply the fitted StandardScaler model to the validation and test sets as well (using the transform --- not the fit --- method). (Look at the notebook we went through in class on regularization and lasso if you are confused.) The transformed datasets are the final datasets you will feed into your logistic regression model. (5 points)"
      ],
      "id": "MWZp2Jw7TKVm"
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "5olW55l8acti"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Transform train set, validation set, and test set using scaler\n",
        "sqf_train = scaler.fit_transform(sqf_train)\n",
        "sqf_validation_2 = scaler.transform(sqf_validation_2)\n",
        "sqf_test = scaler.transform(sqf_test)"
      ],
      "id": "5olW55l8acti"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "involved-implement"
      },
      "source": [
        "**e. Using sklearn.linear_model.LogisticRegression, fit a model on the train set to predict found_weapon. Set the \"penalty\" argument to \"none\" so that the model will not use any regularization; this corresponds to fitting a regular logistic regression model. If you get a `ConvergenceWarning`, increase the number of iterations using the `max_iter` argument; this means the model optimization needs more iterations to converge. (Look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) if you are uncertain which arguments to use!)**\n",
        "\n",
        "A standard measure of predictive performance for binary outcome variables like found_weapon is AUC. Higher values of AUC are better; an AUC of 1 means that the model is perfectly predicting the outcome; an AUC of 0.5 means that it is predicting it only as well as random chance. Report the AUC of the fitted model (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) on both the train and validation sets. What is the problem with using accuracy as a metric for this task? How does the train set AUC differ from the validation AUC, and does this make sense? (7 points)"
      ],
      "id": "involved-implement"
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "6W1lviIFacti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e5f8841-8219-4977-d3e3-ec39fe5c6eaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1500, penalty='none')"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression as logReg\n",
        "from sklearn import metrics\n",
        "\n",
        "# Fit a logistic regression model on found_weapon column\n",
        "found_weapon_reg = logReg(penalty='none', max_iter=1500)\n",
        "found_weapon_reg.fit(sqf_train, train_found_weapon)"
      ],
      "id": "6W1lviIFacti"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the AUC of fitted model using sklearn\n",
        "predicted_val_train = found_weapon_reg.predict_proba(sqf_train)[::,1]\n",
        "train_auc = metrics.roc_auc_score(train_found_weapon, predicted_val_train)\n",
        "print(\"AUC of training set:\", train_auc)\n",
        "\n",
        "# Calculate the AUC of validation set\n",
        "predicted_val_validation = found_weapon_reg.predict_proba(sqf_validation_2)[::,1]\n",
        "validation_auc = metrics.roc_auc_score(validation_found_weapon, predicted_val_validation)\n",
        "print(\"AUC of validation set:\", validation_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD18CGO14y8B",
        "outputId": "04c4b396-c1dc-4c5f-91d9-8b8982f4f1c3"
      },
      "id": "bD18CGO14y8B",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC of training set: 0.8363328538894882\n",
            "AUC of validation set: 0.7498650253752295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ep5iHbBhn1z"
      },
      "source": [
        "The problem with using accuracy as a metric for this task is that the data itself is tinted with discrimination. We believe that there is a correlation between the race of the person who is stopped and frisked and how often they are stopped. In fact, based on our answer to the first question, we already calcualated such positive correlation for Black and Hispanic population, and a negative correlation for White population. \n",
        "As for why we have different AUC for test set and validation set, we could have overfitted our data because of this inherent bias in the dataset, that is why the performance of the validation set is lower."
      ],
      "id": "1Ep5iHbBhn1z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eight-absorption"
      },
      "source": [
        "**f. Our logistic regression model is not using any regularization and appears to be overfitting. We will try to use regularization to reduce overfitting. You will be fitting an L1-penalized logistic regression model using code that looks something like**\n",
        "\n",
        "`LogisticRegression(C=sparsity_param, penalty='l1', solver='liblinear')`\n",
        "\n",
        "(the \"l1\" specifies that we're using an L1 penalty, as discussed in class; the \"liblinear\" solver is an optimizer that works with the L1 penalty. See the logistic regression documentation for more details.) \n",
        "\n",
        "Increase and decrease the amount of regularization using different values of the C parameter, searching logarithmically over at least 20 values in the range from 1e-2 to 1. (Note: we do not recommend defining a variable named \"C\" in your code, because this may cause weird patsy issues, since patsy also uses \"C\". Give the variable another name. Sorry! I complained to the patsy people.) \n",
        "\n",
        "People generally use logarithmic, not linear spacing, for regularization hyperparameters, because it can be hard to know the appropriate scale - so eg [0.001, 0.01, 0.1, 1, 10] not [1, 2, 3, 4, 5]. Here, we want you to generate logarithmically spaced values between 1e-2 to 1. In general, you can generate logarithmically spaced values by searching over 10**X (the double stars mean raise 10 to the power of X) where X is a linearly spaced vector like [1, 2, 3, 4, 5]. In this case, choose X so you get the appropriate endpoints. Another best practice when searching over hyperparameters is to confirm that whichever one you choose is not on the edge of the space.\n",
        "\n",
        "Print out the train set, val set, and test set AUC for each value of of the regularization parameter. Make a plot where the x-axis is the regularization parameter and the y-axis is AUC, with one line for train AUC, one line for val AUC, and one line for test AUC (use plt.semilogx to plot the lines so the x-axis will be logarithmic, making it easier to see the plot). Comment on the trends. Do you see evidence of overfitting? Explain. For the rest of this assignment, use the model with the highest AUC on the validation set. (10 points)\n",
        "\n",
        "In a full analysis, it would make sense to play with other aspects of the model as well: for example, you could try using other forms of regularization (like L1 vs L2) or other classification algorithms besides logistic regression. The basic pattern, though, would be the same: fit the model on the train set, choose models on the val set, and once you've chosen your best model, assess your results (once!) on the test set."
      ],
      "id": "eight-absorption"
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "AVchP4QMactj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2124f8df-824e-454c-b5e8-0c061a2ba6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01       0.01274275 0.01623777 0.02069138 0.02636651 0.03359818\n",
            " 0.04281332 0.05455595 0.06951928 0.08858668 0.11288379 0.14384499\n",
            " 0.18329807 0.23357215 0.29763514 0.37926902 0.48329302 0.61584821\n",
            " 0.78475997 1.        ]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate logarithmically spaced values using numpy\n",
        "log_val = np.logspace(-2, 0, num=20)\n",
        "print(log_val)"
      ],
      "id": "AVchP4QMactj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Arrays to store AUC values\n",
        "auc_train = []\n",
        "auc_validation = []\n",
        "auc_test = []"
      ],
      "metadata": {
        "id": "qemPkuzcDdeV"
      },
      "id": "qemPkuzcDdeV",
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate AUC for train set, validation set, and test set with each log value\n",
        "for val in log_val:\n",
        "  reg_model = logReg(C=val, penalty='l1', solver='liblinear')\n",
        "  reg_model.fit(sqf_train, train_found_weapon)\n",
        "  print(\"\\nLog value: \",val)\n",
        "\n",
        "  # Train set (same steps as question(e)) \n",
        "  predicted_val_train = reg_model.predict_proba(sqf_train)[::,1]\n",
        "  train_auc = metrics.roc_auc_score(train_found_weapon, predicted_val_train)\n",
        "  auc_train.append(train_auc)\n",
        "  print(\"AUC of training set:\", train_auc)\n",
        "\n",
        "  # Validation set (same steps as question(e)) \n",
        "  predicted_val_validation = reg_model.predict_proba(sqf_validation_2)[::,1]\n",
        "  validation_auc = metrics.roc_auc_score(validation_found_weapon, predicted_val_validation)\n",
        "  auc_validation.append(validation_auc)\n",
        "  print(\"AUC of validation set:\", validation_auc)\n",
        "\n",
        "  # Test set (same steps as question(e)) \n",
        "  predicted_val_test = reg_model.predict_proba(sqf_test)[::,1]\n",
        "  test_auc = metrics.roc_auc_score(test_found_weapon, predicted_val_test)\n",
        "  auc_test.append(test_auc)\n",
        "  print(\"AUC of test set:\", test_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_hRbKFK-SP4",
        "outputId": "7a5f261f-ed9a-462d-b94a-0f211fcdf993"
      },
      "id": "t_hRbKFK-SP4",
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Log value:  0.01\n",
            "AUC of training set: 0.7595560649646411\n",
            "AUC of validation set: 0.7803788375590268\n",
            "AUC of test set: 0.7727568380959631\n",
            "\n",
            "Log value:  0.012742749857031334\n",
            "AUC of training set: 0.7696676854848377\n",
            "AUC of validation set: 0.7857686047705958\n",
            "AUC of test set: 0.7742718083674751\n",
            "\n",
            "Log value:  0.016237767391887217\n",
            "AUC of training set: 0.77941511596548\n",
            "AUC of validation set: 0.7884680972660066\n",
            "AUC of test set: 0.7764511498549598\n",
            "\n",
            "Log value:  0.0206913808111479\n",
            "AUC of training set: 0.7876336075152823\n",
            "AUC of validation set: 0.7881895886500168\n",
            "AUC of test set: 0.776894630385315\n",
            "\n",
            "Log value:  0.026366508987303583\n",
            "AUC of training set: 0.7973991669663192\n",
            "AUC of validation set: 0.7864833118690759\n",
            "AUC of test set: 0.7760158252883813\n",
            "\n",
            "Log value:  0.03359818286283781\n",
            "AUC of training set: 0.8061952010667626\n",
            "AUC of validation set: 0.7885895744283002\n",
            "AUC of test set: 0.776542564615623\n",
            "\n",
            "Log value:  0.04281332398719394\n",
            "AUC of training set: 0.8137878685724559\n",
            "AUC of validation set: 0.7897770219198791\n",
            "AUC of test set: 0.77769085634901\n",
            "\n",
            "Log value:  0.0545559478116852\n",
            "AUC of training set: 0.8193534250269687\n",
            "AUC of validation set: 0.7895373596593108\n",
            "AUC of test set: 0.7778961147707222\n",
            "\n",
            "Log value:  0.06951927961775606\n",
            "AUC of training set: 0.8235151848855329\n",
            "AUC of validation set: 0.7884516369459127\n",
            "AUC of test set: 0.7768188478885569\n",
            "\n",
            "Log value:  0.08858667904100823\n",
            "AUC of training set: 0.8260732799952056\n",
            "AUC of validation set: 0.7871611478505455\n",
            "AUC of test set: 0.7753541727270009\n",
            "\n",
            "Log value:  0.11288378916846889\n",
            "AUC of training set: 0.8281397578808584\n",
            "AUC of validation set: 0.7858459682750375\n",
            "AUC of test set: 0.7739459096482333\n",
            "\n",
            "Log value:  0.14384498882876628\n",
            "AUC of training set: 0.829575842023253\n",
            "AUC of validation set: 0.7839635660690912\n",
            "AUC of test set: 0.7723908392214862\n",
            "\n",
            "Log value:  0.18329807108324356\n",
            "AUC of training set: 0.8306058971592952\n",
            "AUC of validation set: 0.7812923853242422\n",
            "AUC of test set: 0.7707752787300621\n",
            "\n",
            "Log value:  0.23357214690901212\n",
            "AUC of training set: 0.8315330966079348\n",
            "AUC of validation set: 0.7784315816919102\n",
            "AUC of test set: 0.7696769422748071\n",
            "\n",
            "Log value:  0.29763514416313175\n",
            "AUC of training set: 0.8325361081145871\n",
            "AUC of validation set: 0.7755688028211671\n",
            "AUC of test set: 0.7683604337885268\n",
            "\n",
            "Log value:  0.37926901907322497\n",
            "AUC of training set: 0.8333106346637901\n",
            "AUC of validation set: 0.7729009141403368\n",
            "AUC of test set: 0.7665797150306256\n",
            "\n",
            "Log value:  0.4832930238571752\n",
            "AUC of training set: 0.8338784010547764\n",
            "AUC of validation set: 0.770364708020258\n",
            "AUC of test set: 0.7646725788341865\n",
            "\n",
            "Log value:  0.615848211066026\n",
            "AUC of training set: 0.8343872108354309\n",
            "AUC of validation set: 0.7679990308163529\n",
            "AUC of test set: 0.7626641727541874\n",
            "\n",
            "Log value:  0.7847599703514611\n",
            "AUC of training set: 0.8347379539733908\n",
            "AUC of validation set: 0.7663292959460205\n",
            "AUC of test set: 0.7608997659238396\n",
            "\n",
            "Log value:  1.0\n",
            "AUC of training set: 0.834973405849215\n",
            "AUC of validation set: 0.7643613000755857\n",
            "AUC of test set: 0.7592298323405712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the AUC values of train, validation, and test set against the regularization parameter log values\n",
        "plt.semilogx(log_val, auc_train, label='Train Set')\n",
        "plt.semilogx(log_val, auc_validation, label='Validation Set')\n",
        "plt.semilogx(log_val, auc_test, label='Test Set')\n",
        "plt.xlabel(\"Regularization Parameter\")\n",
        "plt.ylabel(\"AUC Value\")\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "cPSQCvFZFiOT",
        "outputId": "7afe5ec8-d65d-426f-8fec-9c0c7429d0b9"
      },
      "id": "cPSQCvFZFiOT",
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(*args, **kw)>"
            ]
          },
          "metadata": {},
          "execution_count": 125
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owMXSAmTeInj"
      },
      "source": [
        "Yes, we see evidence of overfitting. The AUC value of the training set increases quickly before around the value of 0.08. After that, it still continues to climb, but very slowly. On the other hand, for the test set, AUC starts to quickly go down after 0.08. For the balidation set, AUC starts to go down after about 0.03. All of these demonstrate that we have overfitted our model after a log value in betwen 0.03 and 0.08 - the performance of the train set keeps improving, but it does not generalize to validation and test set."
      ],
      "id": "owMXSAmTeInj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Find out the max AUC value of the validation set and the corresponding log value\n",
        "max_val_auc = max(auc_validation)\n",
        "max_val_log = log_val[auc_validation.index(max_val_auc)]\n",
        "print(\"Maximum AUC: \", max_val_auc, \"\\nCorrespinding log value: \", max_val_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWPO0G4hRBPZ",
        "outputId": "34ffab83-7c41-4e76-97b4-9724c77d52f8"
      },
      "id": "MWPO0G4hRBPZ",
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum AUC:  0.7897770219198791 \n",
            "Correspinding log value:  0.04281332398719394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "antique-connection"
      },
      "source": [
        "**g. Assess model *calibration* on the test set. This checks whether the model's predicted probabilities line up with the true probabilities, and is important to assess here because we will be analyzing the model's predicted probabilities. To assess calibration, take the 10% of rows of the test set with the highest model predictions and compare the mean model predicted probability (ie, the output of `predict_proba`) to the actual mean of the outcome variable. Repeat for the next 10% of rows, and for all 10% groups. Make a plot comparing the model predicted probabilities on each 10% group to the actual outcomes (mean predicted probability on the x-axis and mean actual outcome on the y-axis). You should end up with a plot with 10 points, one for each 10% group. Plot the line y=x so you can see how well the predicted probabilities line up with the actual probabilities - ideally, your points will lie on the line or close to it. (10 points)**\n",
        "\n",
        "Calibration is an issue for many machine learning models, including deep learning models, so this is always a good thing to check. In a full analysis, it would make sense to check calibration (and AUC) for subgroups as well (eg, each race group). "
      ],
      "id": "antique-connection"
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "CAFciglqactj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "8c650c85-5f95-4ab5-c16b-dc33f25a8dd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Predicted Value  found_weapon\n",
              "57             0.014667         False\n",
              "149            0.025733         False\n",
              "211            0.022933         False\n",
              "225            0.035491         False\n",
              "241            0.051532         False\n",
              "...                 ...           ...\n",
              "204201         0.555000          True\n",
              "204217         0.023492         False\n",
              "204238         0.028642         False\n",
              "204251         0.023593         False\n",
              "204253         0.011798         False\n",
              "\n",
              "[6420 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a23a0455-9762-4994-a74d-68dea406e6fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted Value</th>\n",
              "      <th>found_weapon</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.014667</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>0.025733</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>0.022933</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>0.035491</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>0.051532</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204201</th>\n",
              "      <td>0.555000</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204217</th>\n",
              "      <td>0.023492</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204238</th>\n",
              "      <td>0.028642</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204251</th>\n",
              "      <td>0.023593</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204253</th>\n",
              "      <td>0.011798</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6420 rows  2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a23a0455-9762-4994-a74d-68dea406e6fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a23a0455-9762-4994-a74d-68dea406e6fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a23a0455-9762-4994-a74d-68dea406e6fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "# Fit a new logistic regression model with parameters from the highest AUC on validation set\n",
        "reg_model_new = logReg(C=max_val_log, penalty='l1', solver='liblinear')\n",
        "reg_model_new.fit(sqf_train, train_found_weapon)\n",
        "\n",
        "# Predict with test set\n",
        "test_predicted = reg_model_new.predict_proba(sqf_test)[::,1]\n",
        "\n",
        "# Create a new dataframe combining prediction and true values\n",
        "predicted_set = pd.DataFrame({\"Predicted Value\":test_predicted}, index=test_found_weapon.index)\n",
        "test_true = pd.concat([predicted_set, test_found_weapon], axis=1)\n",
        "test_true"
      ],
      "id": "CAFciglqactj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode found_weapon column to numerical values and sort model predictions from highest to lowest\n",
        "test_true['found_weapon'] = test_true['found_weapon'].astype(int)\n",
        "test_true = test_true.sort_values(by=['Predicted Value'], ascending=False)\n",
        "test_true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3E6xzyBjYFBG",
        "outputId": "56147832-e99d-4745-ddfb-cdafb2bcc0ea"
      },
      "id": "3E6xzyBjYFBG",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Predicted Value  found_weapon\n",
              "142568         0.955049             1\n",
              "203928         0.912134             1\n",
              "145642         0.905097             1\n",
              "70601          0.884284             1\n",
              "162249         0.867473             0\n",
              "...                 ...           ...\n",
              "120684         0.003860             0\n",
              "80774          0.003787             0\n",
              "64701          0.003744             0\n",
              "1053           0.002528             0\n",
              "156040         0.002368             0\n",
              "\n",
              "[6420 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5cc8ccd3-8fa4-4a39-8203-2febb9d99b42\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted Value</th>\n",
              "      <th>found_weapon</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>142568</th>\n",
              "      <td>0.955049</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203928</th>\n",
              "      <td>0.912134</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145642</th>\n",
              "      <td>0.905097</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70601</th>\n",
              "      <td>0.884284</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162249</th>\n",
              "      <td>0.867473</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120684</th>\n",
              "      <td>0.003860</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80774</th>\n",
              "      <td>0.003787</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64701</th>\n",
              "      <td>0.003744</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1053</th>\n",
              "      <td>0.002528</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156040</th>\n",
              "      <td>0.002368</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6420 rows  2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5cc8ccd3-8fa4-4a39-8203-2febb9d99b42')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5cc8ccd3-8fa4-4a39-8203-2febb9d99b42 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5cc8ccd3-8fa4-4a39-8203-2febb9d99b42');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two empty arrays to store mean of predicted probability and actual probability\n",
        "copy_test_true = test_true.copy()\n",
        "predicted_mean = []\n",
        "true_mean = []\n",
        "\n",
        "# Calculate the number of 10% of rows\n",
        "row = copy_test_true.shape[0]//10\n",
        "\n",
        "# For every 10% of rows, calculate the mean of predicted probability and actual probability, and store in corresponding array\n",
        "for i in range(0, copy_test_true.shape[0] + 1, row):\n",
        "  calib_10 = copy_test_true.head(i)\n",
        "  mean_predicted = calib_10['Predicted Value'].mean()\n",
        "  predicted_mean.append(mean_predicted)\n",
        "  mean_true = calib_10['found_weapon'].mean()\n",
        "  true_mean.append(mean_true)"
      ],
      "metadata": {
        "id": "sxR0SlCfThvO"
      },
      "id": "sxR0SlCfThvO",
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the first null value of the mean array\n",
        "predicted_mean = predicted_mean[1:11]\n",
        "true_mean = true_mean[1:11]\n",
        "print(len(predicted_mean))\n",
        "print(len(true_mean))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP16SBtUkWyh",
        "outputId": "25f4214d-f9d2-416d-edc6-3ea09e399413"
      },
      "id": "MP16SBtUkWyh",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot mean predicted probability against mean actual outcome\n",
        "fig, ax = plt.subplots()\n",
        "plt.scatter(predicted_mean,true_mean)\n",
        "\n",
        "# (I referenced a Stack Overflow answer on adding y=x to a matplotlib plot: https://stackoverflow.com/questions/25497402/adding-y-x-to-a-matplotlib-scatter-plot-if-i-havent-kept-track-of-all-the-data)\n",
        "lims = [\n",
        "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
        "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
        "]\n",
        "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
        "ax.set_aspect('equal')\n",
        "ax.set_xlim(lims)\n",
        "ax.set_ylim(lims)\n",
        "\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Actual Probability')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "AcJK2R-cd9Iw",
        "outputId": "10fb3e33-f7ac-433a-e475-1c79e247f27d"
      },
      "id": "AcJK2R-cd9Iw",
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frozen-apollo"
      },
      "source": [
        "**h. Using the test set, compute the fraction of observations which have lower than a 2% model-predicted probability of finding a weapon for white, black, Hispanic, and Asian pedestrians. Stops below this threshold are extremely unlikely to have resulted in finding a weapon, arguably violating the Fourth Amendment. Repeat this for 20 thresholds evenly spaced between 1% and 5%. Make a graph where the x-axis is the threshold, and the y-axis is the fraction of stops falling below that threshold, with one line for each race group. Explain what you observe. Do you think this graph provides evidence for Fourteenth Amendment violations (racial discrimination)? (10 points)**"
      ],
      "id": "frozen-apollo"
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "_Zj-pRkFactj"
      },
      "outputs": [],
      "source": [
        "# Extract suspect_race from dataframe\n",
        "df2_race = df2_dropped['suspect_race'].loc[sqf_test_copy.index.values]"
      ],
      "id": "_Zj-pRkFactj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcualte the fraction of observations that have lower than threshold probability of finding a weapon\n",
        "def calculateFraction(race, threshold):\n",
        "  num_stops = df2_race[df2_race == race].shape[0] # Total number of stops for a certain race\n",
        "  stops = test_true[test_true['Predicted Value'] < threshold] # Filter for those predictions below threshold\n",
        "  suspect_race = df2_dropped.loc[stops.index, 'suspect_race']\n",
        "  race_based_stop = suspect_race[suspect_race == race].shape[0] # Stops that have lower than threshold percentage of predicted probabability of finding a weapon\n",
        "  fraction = race_based_stop/ num_stops\n",
        "  return fraction"
      ],
      "metadata": {
        "id": "cd-JNZFkqlYn"
      },
      "id": "cd-JNZFkqlYn",
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fraction of stops with lower than 2% threshold probabability of finding a weapon. Breakdown by race\n",
        "print(\"White: \", calculateFraction('white', 0.02))\n",
        "print(\"Black: \", calculateFraction('black', 0.02) )\n",
        "print(\"Hispanic: \", calculateFraction('hispanic', 0.02) )\n",
        "print(\"Asian: \", calculateFraction('asian', 0.02) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sszpmMq1txxZ",
        "outputId": "fd6d996e-bceb-4e08-cd84-328f8063a21c"
      },
      "id": "sszpmMq1txxZ",
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "White:  0.06896551724137931\n",
            "Black:  0.5672601384767557\n",
            "Hispanic:  0.305\n",
            "Asian:  0.38372093023255816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find 20 thresholds evenly spaced between 1% and 5%\n",
        "thresholds = np.linspace(0.01, 0.05, num=20, endpoint=True)\n",
        "\n",
        "white_frac = []\n",
        "black_frac = []\n",
        "hispanic_frac = []\n",
        "asian_frac = []\n",
        "\n",
        "# Calculate fraction for each threshold value and append the value to arrays\n",
        "for thres in thresholds:\n",
        "  white_frac.append(calculateFraction('white', thres))\n",
        "  black_frac.append(calculateFraction('black', thres))\n",
        "  hispanic_frac.append(calculateFraction('hispanic', thres))\n",
        "  asian_frac.append(calculateFraction('asian', thres))\n",
        "\n",
        "# Plot the fraction of stops falling below threshol against threshold, with one line for each race group\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(thresholds, white_frac, label = \"White\")\n",
        "ax.plot(thresholds, black_frac, label = \"Black\")\n",
        "ax.plot(thresholds, hispanic_frac, label = \"Hispanic\")\n",
        "ax.plot(thresholds, asian_frac, label = \"Asian\")\n",
        "ax.set_xlabel(\"Threshold Value\")\n",
        "ax.set_ylabel(\"Fraction of Stops Below Threshold\")\n",
        "ax.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "7aqXwVjoubnK",
        "outputId": "4f731cf0-74ae-4c28-8fb0-9cb66227c27c"
      },
      "id": "7aqXwVjoubnK",
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fecce704610>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DutLGn3nactj"
      },
      "source": [
        "From the graph, we see that White pedestrians have the lowest rate of being stopped, and Black pedestrians have the highest rate of being stopped regardless of the threshold. <br>\n",
        "This graph is evidence for Fourteenth Amendment violations (racial discrimination) for the following reasons: <br>\n",
        "1) No matter if the threshold is 2% or 5%, White pedestrians have a much lower rate of being stopped and frisked compared to other populations. When we look closely at the 2% threshold, all three other races have almost double or triple the fraction of being stopped, showing that the police's decision to stop and frisk is highly racially motivated and biased towards White population. <br>\n",
        "2) As the threshold value increases, the line for White pedestrains increases at a much slower rate compared to Black and Hispanic populations. This indicates that even as the probability of finding a weapon becomes higher, police is still less inclined to stop White population and more willing to stop Black, Hispanic, and Asian population. "
      ],
      "id": "DutLGn3nactj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "organized-luxembourg"
      },
      "source": [
        "**i. The analysis you have performed in the second part of this assignment is very similar to the analysis in [this paper](https://5harad.com/papers/stop-and-frisk.pdf). Read the paper and write a few sentences about their main conclusions. (5 points)**"
      ],
      "id": "organized-luxembourg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wMU2Jzse3Dm"
      },
      "source": [
        "The paper draws these main conclusions: <br>\n",
        "1) The authors of the paper found out that in \"43% of the CPW stops between 2011 and 2012, there was only 1% chance of finding a weapon on the suspect\", suggesting that individuals were often stopped with little evidence of criminal activity. <br>\n",
        "2) Black and Hispanics were disproportionately involved in low hit rate stops compared to Whites, even after correcting for factors such as highly localized policing tactics. <br>\n",
        "3) The authors suggest that one can \"recover 50% of weapons by conducting only the 6% of CPW stops with the highest ex ante hit rate, and 90% of weapons by conducting 58% of CPW stops\". They also developed a set of stop heuristics that can help police officers achieve such statistical efficacy via a simple scoring rule."
      ],
      "id": "6wMU2Jzse3Dm"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}